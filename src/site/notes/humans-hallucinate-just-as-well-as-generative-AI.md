---
{"dg-publish":true,"permalink":"/humans-hallucinate-just-as-well-as-generative-ai/"}
---

#writing/blog

# Hallucinating AI

One of the more peculiar behaviours that has emerged form [[GenAI\|Generative AI]] has been the phenomena of [[AI hallucinations\|AI hallucinations]], where the system creates output that is demonstrably at odds with reality. It can [fabricate answers, complete with references](https://flyingbisons.com/blog/hallucinations-of-chatgpt-4-even-the-most-powerful-tool-has-a-weakness), to prompted questions. This is in part because it (excuse the anthropomorphism) *wants to please you*. Given the text of a question, it seeks to continue this into an answer, even if the question is counterfactual. Some examples have been confirming the "[sole survivor of the Titanic](https://flyingbisons.com/blog/hallucinations-of-chatgpt-4-even-the-most-powerful-tool-has-a-weakness)" or drawing a perfectly normal cat with 6 legs, or [wolf pups that merge and re-emerge from themselves](https://www.washingtonpost.com/technology/interactive/2024/ai-video-sora-openai-flaws/).

This behaviour is, in a sense, a design feature of Generative AI. 

> It has built an internal model of the world, based on vast amounts of data, and it uses this model to generate a more complete picture from what it 'observes' in the form of the prompt. 
 
As such, it sometimes forms a picture of the world that is not really there. This seems strange, possibly a little alien, but we humans should hold off on any harsh judgement on this kind of behaviour. 


# The human visual system

We too have built many internal models of the world and utilise them constantly. One such model is [of colour](https://en.wikipedia.org/wiki/Color_vision). Our eyes receive information about ranges of light wavelengths, essentially the colours red, green and blue. It is our model of the world, facilitated by our brain, that then takes this data and extrapolates it into a range of colours. Sound familiar? 

> We have a built in model of the world, based on vast amounts of data (experience), and we use this model to generate a more complete picture from what we observe in the form of sensory input. 

But unlike the AI hallucination, we cannot see the raw sensory input in a way that we can distinguish it from the extrapolation. In a way, the weird behaviour of GenAI hallucination helps highlight how weird our experience of reality already is. But there is a key difference. 

# Uncertainty and reality tests

When our models behave inconsistently we experience a familiar emotion - doubt. Did I really see that? Our brain then attends to this problem in order to correct, and performs a [reality test](https://www.adelaide.edu.au/news/news68562.html#:~:text=Hallucination%3A%20The%20apparent%20perception%20of,erroneous%20%22sense%20of%20familiarity%22.). We have an inbuilt epistemic humility that the AI, as yet, does not. 

Thomas G. Dietterich [frames  this well](https://www.youtube.com/watch?v=cEyHsMzbZBs&ab_channel=valgrAI) as two kinds of uncertainty - [[epistemic uncertainty\|epistemic uncertainty]] (I'm not sure I know enough of the facts to decide who the murderer is) or [[aleotoric uncertainty\|aleotoric uncertainty]] (I don't know if that falling leaf will land in my tea). Aleotoric uncertainty is probabilistic uncertainty, the unknowable roll of the dice, and is the only uncertainty that the GenAI knows. When this kind of uncertainty is present it can be reasonable to make a 'best guess'. But when we have epistemic uncertainty, the uncertainty of making sense of what is know, this is were we should say 'I don't know'.

All that said, I'm just appreciative that this train of thought means that next time I see a rainbow I'm going to appreciate it even more, knowing that I am hallucinating most of it. 
