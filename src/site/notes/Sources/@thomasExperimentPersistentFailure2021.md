---
{"dg-publish":true,"permalink":"/sources/thomas-experiment-persistent-failure2021/","title":"Experimentâ€™s persistent failure in education inquiry, and why it keeps failing","tags":["ðŸ“–"]}
---


Thomas, G. (2021). Experimentâ€™s persistent failure in education inquiry, and why it keeps failing. _British Educational Research Journal_, _47_(3), 501â€“519. [https://doi.org/10.1002/berj.3660](https://doi.org/10.1002/berj.3660)
[online](http://zotero.org/users/5872672/items/JB83I3QH) [local](zotero://select/library/items/JB83I3QH) [pdf](file:///Users/14055622/Zotero/storage/NXINQTUQ/Thomas%20-%202021%20-%20Experimentâ€™s%20persistent%20failure%20in%20education%20inqui.pdf)
 
# Blog post 
There is a [blog post](https://www.bera.ac.uk/blog/what-can-experiment-tell-us-in-educational-research) outlining this paper.

# Background

Gary Thomas is with the [Uni of Birmingham](https://www.birmingham.ac.uk/staff/profiles/education/thomas-gary), school of education, with a particular focus on Inclusion and Diversity. With this focus, it's not surprising he has written something related to 'persistent failure' of our interventions in education, since these issues remain highly relevant today. 

Being of a nervous disposition as a child, Gary Thomas failed to write anything on his 11-plus examination paper, which inaction took him to secondary modern school. His subsequent zigzag through the education system gave him broad experience of its good and bad sides.

He eventually became a teacher, then an educational psychologist, then a professor of education at the University of Birmingham (and four previous universities) where his teaching, research and writing now focus on inclusive education and the methods used in social science research.

He has led a wide range of research projects and has received awards from the AHRC, the ESRC, the Nuffield Foundation, the Leverhulme Trust, the Department for Education, charities such as Barnardos and the Cadmean Trust, local authorities and a range of other organisations. He has written or edited more than 20 books and lots of boring academic articles.

His recent research and scholarship have centred on inclusive education, on the work of additional personnel in schools and research methodology in education, with a particular focus on the epistemology of special education.

Gary Thomas is currently ...

i) leading an AHRC funded research project entitledÂ [The changing nature of 'connectivity' within and between communities](http://connectedcomms.wordpress.com/ "The changing nature of 'connectivity' within and between communities").Â 

ii) a co-investigator on the Sense-funded project (PI Liz Hodges)Â [The experiences of diagnosis for people with Usher syndrome](https://www.birmingham.ac.uk/research/victar/research/usher-syndrome "The experiences of diagnosis for people with Usher syndrome")

# Themes

- Use of natural metaphor, comparisons to the natural sciences
- Digging into the *why* of gathering evidence

# Key messages

> [!quote] Key message of [[Sources/@thomasExperimentPersistentFailure2021\|@thomasExperimentPersistentFailure2021]]
> My case is that we havenâ€™t as a community of inquiry learned the lessons of the past, and the putative improvement and purification of the experiment form is once again leaving us disappointed. Worse, it has complicated and etiolated education researchâ€”we always seem to be stretching up for some purer form, to the extent that inquiry is circumscribed, and with that circumscription, distorted and enfeebled.

Call to arms:

> [!quote] Call to action about how we make claims
> Scientists are catholic in their attitude to inquiry and experimentâ€”their attitude to inquiry is fluid, flexible, protean. And their work is successful: I gave the example of the methodological eclecticism of palaeoanthropology earlier ... and huge strides have been made in palaeoanthropology and in most sciences in recent years. But few would claim similar strides in education science.

> [!quote] A broader view of experiment
> Randomisation, the proffered solution to the problems of the 1960s/1970s tranche of experiments, in reality provides no meaningful change. My contention is that what I have called the â€˜third waveâ€™ of randomised experimentsâ€”just like the second, mainly unrandomised wave in the 1960s and 1970sâ€”is yielding â€˜small and uninformative effectsâ€™ (Lortie-Forgues & Inglis, 2019) because this model of intervene-and-test using the protocols of a particular kind of experiment is, in education, flawed, unable to meet this branch of experimentationâ€™s own design expectations and unwilling to take seriously the significance of confounders which vitiate the legitimacy of its findings. This flawed rendition of experiment is giving misleading policy advice as models for change which may be successful in some circumstances are rejected on the basis of low effectiveness scores, while others in which potential effectiveness is indicated are unproductively imposed where circumstances are unpropitious. 
> I suggest that a more unrestricted interpretation of â€˜experimentâ€™ needs to return to education discourse.

**etiolated**: (of a plant) pale and drawn out due to a lack of light
**putative**: accepted / assumed 
**catachresis**: use of a word in an incorrect way
**catholic**: means 'universal' with a lower case-c. I had no idea. 

# Notes

## History towards [[RCTs\|RCTs]]

Outlines the cyclical / faddish nature of our fascination of RCTs. Talks about being wedded to what Parlett and Hamilton (1972) call the 'classical' or 'agricultural-botany' paradigm "which utilizes a hypothetico-deductive methodology derived from the experimental and mental-testing traditions in psychology" (from Parlett and Hamilton, 1972: Evaluation as Illumination: A new approach to the study of innovatory programs).

Key point is *why* it started:

> [!quote] Why did we want improved evidence?
> Out of a desire to achieve respectability in a world where the natural sciences were realising extraordinary feats, and where social sciences, notably psychology, were modestly successful in emulating them (at least in laboratory settings), educators sought what they assumed to be similar methods of verification for their theories

Is there, a [[curse-of-experimental-success-in-other-fields-afflicting-education\|curse-of-experimental-success-in-other-fields-afflicting-education]]? It has been successful in perceived "hard" sciences, why not the perceived easier "soft" sciences? Importantly, we care about education, and what to share in these amazing advances. However the "hard" sciences are often easier than the "soft" sciences. 

I love the framing of the "discovery" of evidence! 

> [!quote] On the faddish nature of 'evidence'
> A new fashion for experimentation took hold (Wrigley, 2018: 14 calls it a â€˜cultâ€™), and with it came new phrasesâ€”â€˜evidence-basedâ€™ and â€˜What Worksâ€™ (Thomas, 2021)â€”and the establishment of national bodies committed to discovering What Worksâ€”the What Works Clearinghouse (WWC) in the USA and the Education Endowment Foundation (EEF) in the UKâ€”which have funded hundreds of experiments.

> [!quote] Adopting the wrong method
> But for more than a century, educators and social scientists have remained transfixed with what Parlett and Hamilton (1972) called the â€˜agricultural-botany paradigmâ€™, copying the experiment form of one sliver of scientific enterprise used in plant science, a methodology successfully emulated in pharmacology and medicine.

#### Discussion 1 (optional): Allocation bias

> [!quote] Allocation bias should have helped
> But randomisation adds just one ingredient to the earlier formula â€”that ingredient being the supposed elimination of allocation bias to groups via randomisation, as if allocation bias had been the wicked problem afflicting the 1960s/ 1970s tranche of experiments. But allocation bias ought, if it were operating, to have favoured positive findings about intervention.

==Discuss: Why is this?==
Note: allocation bias is the uneven application of a treatment. 

#### Discussion 2: The substrate

> [!quote] From the "The answer lies in the substrate" section. Interesting choice of words. 
> And in trying something out, one lives in oneâ€™s environment. One is guided by that environment; one doesnâ€™t fight it. â€˜Right plant, right place!â€™ was the mantra of horticulturalist Beth Chatto, and weâ€™d do well to learn from her advice, for a field of inquiry (or a field of anything, for that matter) must inhabit a substrate that shapes its form.

Goes on to answer:

> [!quote] 
> One may be able to say that T causes O, but because in an education landscape K1,K2, ...,Kn (e.g. method, class size, teacher personality and style, catchment, novelty and other factors) exist and are significant in any subpopulation, T will rarely be shown to cause O to any significant extent.

In education, the substrate (i.e. the background context) is highly influential. He links this into the Pareto principal or the 'law of the vital few'. This is the 80/20 rule: 80% of the impact comes from 20% of the causes. The key is that these 'vital few' often lie in the confounders. 

note: no reason that needs to add up to 100%, FYI. 

I disagree. Doesn't quite match with a complex systems perspective. 

#### Discussion 3: The trials themselves

> [!quote] 
> I suggest that the problem is deeper than this: trials themselvesâ€”trials qua trialsâ€”are not, most of the time, fit for purpose. It is to this issue that I now turn.

Do we agree?

> [!quote] 
> If most outcomes are determined by a few factors (such as enthusiasm, sensitivity, engagement, amount of help), then these will always in real-life situations counteract and outweigh the impact of independent variables of interest in experimental research (see Biesta, 2015 for an excellent discussion of quasi-causation). So, any effects of imposing a new teaching method are likely to be occluded by the influence of other variables, such as teacher style, school catchment, parent support, enthusiasm, and so on (the â€˜K1,K2, ...,Knâ€™ to which I referred aboveâ€”the â€˜causal cakesâ€™ of which Cartwright & Hardie, 2012 speak), all working to overwhelm the influence of the variable of interest.
> In the worlds of social science, the confounders are bound to winâ€”in Normanâ€™s (2003) terms, drowning treatment effects in a sea of unexplained variance.

Think, why did he highlight control here:
> [!quote] 
> But, highly important though this is, it is separate from the issue of the RCTâ€™s claim, as a particular kind of experiment, to *control* for confounders. There is room for confusion here, as the claim to control for confounders may well be read as an elimination of the confounder issue.

#### Discussion: Then what?

> [!quote] 
> Valid conclusions have to be embedded in a broad range of contextual information that can lead to what is now known by philosophers of science as â€˜inference to the best explanationâ€™ (IBE) (Harman, 1965; Okasha, 2002; Lipton, 2004), to which I shall return in a moment.

> [!quote] 
> As Cartwright suggests, the agricultural/botany form of the experiment is not the only way of identifying cause. She says that the proclaimed attribute of cause-establishment is sometimes, but only sometimes, a property of the experiment, but this quality is not unique to work using experiment. She gives examples of other forms of cause-establishment, including economistsâ€™ use of modelling to estimate the degree to which one factor predicts another in a given population (â€˜probabilistic/Granger causalityâ€™), where, given the right assumptions, results can legitimately imply causal conclusions.
> Scriven (2008: 22â€“23) concurs. In explaining a process he calls â€˜general elimination methodologyâ€™ (GEM), he describes something very similar to IBE. As examples of analysis where GEM is involved, he suggests that the identification of causal route can come from processes as simple as direct critical observation, with direct or simple inductive inference, as in much astronomy, autopsy or engineering breakdown. He also notes the significance of inference based on use of analogy or theory, as in geology. He asserts that inference comes quite validly also from simple direct manipulation, whether it is in the laboratory or the kitchen. Science also makes valid inferences from what he calls â€˜natural experimentsâ€™, as in meteorology and epidemiology.

> [!quote] 
> IBE is entirely in tune with the realist evaluation of Pawson and Tilley (1997) in asking not â€˜What works?â€™, but rather â€˜What works for whom in what circumstances and in what respects, and how?â€™ Whether itâ€™s called action research, case study, ethnography or something else, much supposedly non-experiment education inquiry does this kind of thing.

> [!quote] 
> In each, the researcher observes the effects of naturally occurring changes, triangulating forms of data collection and forms of analysis to emerge with invaluable insights about the workings of schools. It seems inappropriate to me to exclude these inquiries from the designation â€˜experimentâ€™: in essence, the working methods of these researchers are akin to those of most scientists, whether they be astrophysicists, epidemiologists, meteorologists, palaeo-anthropologists or zoologists. 
> For both the teacher and the natural scientist, using a mix of methods of inquiry--Haackâ€™s (2007) â€˜loose federation of kinds of inquiryâ€™ (p. iv)â€”comes naturally, as long as certain shibboleths of experiment methodology in the Fisherâ€“Campbellâ€“Stanley tradition are relinquished. There is much to be gained from returning to more straightforwardly appropriate ideas of what might constitute an experiment in education inquiry.


#### Discussion extras: My thoughts

- Time is rubbery in education. Treatment - wait x - Effect might not be the correct thinking. 

### Connections

supports:: 
refutes:: 

### Notes